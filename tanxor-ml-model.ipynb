{"cells":[{"cell_type":"markdown","metadata":{},"source":["#### Importing Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-29T05:17:25.325658Z","iopub.status.busy":"2023-07-29T05:17:25.324445Z","iopub.status.idle":"2023-07-29T05:17:25.363494Z","shell.execute_reply":"2023-07-29T05:17:25.362335Z","shell.execute_reply.started":"2023-07-29T05:17:25.325610Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import json\n","\n","from statsmodels.tsa.stattools import adfuller\n","from matplotlib import pyplot as plt\n","\n","from preprocessing.ImputeMean import ImputeMean\n","from preprocessing.TrainTestSplit import TrainTestSplit\n","from preprocessing.ZeroSales import ZeroSales\n","from preprocessing.DataAggregator import DataAggregator\n","from preprocessing.FeatureEngineering import Lag, Log\n","\n","from model.Train import SRX, RFR\n","from model.Optimize import SarimaxHyperopt, RFR_Optuna\n","from model.Evaluate import Evaluate\n","\n","from joblib import Parallel, delayed\n","\n","# Read data\n","calendar_df = pd.read_csv('E:/Documents/TanXor/Dataset/calendar.csv')\n","sales_df = pd.read_csv('E:/Documents/TanXor/Dataset/sales_train_validation.csv')\n","validation_df = pd.read_csv('E:/Documents/TanXor/Dataset/sales_train_evaluation.csv')"]},{"cell_type":"markdown","metadata":{},"source":["#### Data Transformation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initializing Parameters\n","date = calendar_df['date'].iloc[:1941]\n","col1, col2 = 'store_id', 'dept_id'\n","\n","data = DataAggregator(validation_df)\n","\n","# Takes col1 and col2 and aggregates them into a new column\n","data.aggregate(col1, col2)\n","\n","# Drops passed columns\n","data.drop(col1)\n","\n","# Groups by the new column\n","data.group_by()\n","\n","# Transforms the dataframe using '.T' function\n","data.transform()\n","\n","# Sets the index to the date column\n","data = data.set_datetime_index(date)\n","\n","# Returns a dataframe with the number of zero sales for each store and department\n","zero_neg = ZeroSales(data).zero_sales()\n","\n","# Replace zero sales with the mean of sales of that respective store and department\n","ImputeMean(data, 0).imputer()\n","\n","#Split data into evaluation and validation sets\n","eval_data, val_data = data.iloc[:1913, :], data.iloc[1913:, :]\n","\n","# Splits the data into train and test sets\n","train_data, test_data = TrainTestSplit(eval_data, 0.2).data_split()\n","\n","# Sets the frequency of the data to daily\n","eval_data.index.freq = val_data.index.freq = train_data.index.freq = test_data.index.freq = 'd'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### For different datasets with same features ###\n","'''\n","df_instances = []\n","datasets = [sales_df, validation_df]\n","dates = [calendar_df['date'].iloc[:1913], calendar_df['date'].iloc[:1941]]\n","\n","for i in range(2):\n","    df = DataAggregator(datasets[i])\n","    df.aggregate(col1, col2)\n","    df.group_by()\n","    df.transform()\n","    df.set_datetime_index(dates[i])\n","    ImputeMean(df.data, 0).imputer()\n","    df_instances.append(df)\n","\n","sales, valid_df = df_instances[0].data, df_instances[1].data\n","val_df = valid_df.iloc[1913:, :]\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def save_to_json(filename, data, mode='w'):\n","    with open(f\"./best_params/{filename}.json\", mode) as f:\n","        json.dump(data, f, indent=4)\n","\n","def read_from_json(filename,mode=\"r\"):\n","    with open(f'./best_params/{filename}.json', mode) as f:\n","        # Load the JSON data into a Python dictionary\n","        params_data = json.load(f)\n","\n","        return params_data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def hyperparameter_tuning(train_data, test_data, col, evals = 3):\n","    params_dict = [{}, {}]\n","\n","    srx_hyperopt = SarimaxHyperopt(train_data[col], test_data[col])\n","\n","    rfr_optuna = RFR_Optuna(train_data[[col]], 7 , 0.2)\n","\n","    models = [srx_hyperopt, rfr_optuna]\n","\n","    for i, model in enumerate(models):\n","\n","        model.hyperparameter_tune(evals)\n","\n","        params_dict[i][col] = model.best_params\n","\n","    return params_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["output = [{}, {}]\n","\n","for i, col in enumerate(train_data.columns[:2]):\n","    output[i] = hyperparameter_tuning(train_data, test_data, col, 2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["output = Parallel(n_jobs=-1)(\n","delayed(hyperparameter_tuning)(train_data, test_data, col, 2)\n","for col in train_data.columns[:20])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["srx_best_ = output[0][0]\n","rfr_best_ = output[0][1]\n","\n","for i in range(1, len(output)):\n","    srx_best_.update(output[i][0])\n","    rfr_best_.update(output[i][1])\n","\n","params_path = ['sarimax_best_params', 'rfr_best_params']\n","params_data = [srx_best_, rfr_best_]\n","\n","for i, path in enumerate(params_path):\n","    save_to_json(path, params_data[i], 'w')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["srx_params = {}\n","rfr_params = {}\n","\n","params = [srx_params, rfr_params]\n","paths = ['sarimax_best_params', 'rfr_best_params']\n","model_names = ['SARIMAX', 'Random_Forest']\n","\n","for i, path in enumerate(paths):\n","    params[i] =  read_from_json(path, mode=\"r\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def model_training(eval_data, val_data, data, col, params, model_names, total_forecast):\n","\n","    total_forecast[f'Original_{col}'] = val_data[col]\n","\n","    srx_model = SRX(eval_data[col], val_data[[col]])\n","\n","    rfr_model = RFR(data[[col]])\n","\n","    rfr_model.data_preprocess(7)\n","\n","    rfr_model.train_test_split(len(val_data)/len(data))\n","\n","    models = [srx_model, rfr_model]\n","\n","    for i, model in enumerate(models):\n","\n","        model.fit(params[i][col])\n","\n","        forecast = model.predict()\n","\n","        total_forecast[f'{model_names[i]}_{col}'] = forecast\n","    \n","    return total_forecast"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["forecast = pd.DataFrame()\n","\n","for col in train_data.columns[:2]:\n","    forecast = model_training(eval_data, val_data, data, col, params, model_names, forecast)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["forecast = pd.DataFrame()\n","\n","forecast = Parallel(n_jobs=-1)(\n","delayed(model_training)(eval_data, val_data, data, col, params, model_names, forecast)\n","for col in train_data.columns[:2])\n","\n","forecast = pd.concat(forecast, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sarimax_forecast = forecast[[col for col in forecast.columns if \"SARIMAX\" in col]].copy()\n","randomforest_forecast = forecast[[col for col in forecast.columns if \"Random\" in col]].copy()\n","\n","sarimax_forecast['Total'] = sarimax_forecast.sum(axis=1)\n","randomforest_forecast['Total'] = randomforest_forecast.sum(axis=1)\n","\n","forecast.to_csv(\"E:/Documents/TanXor/Model/forecasted_data/total_forecast.csv\", header=True)\n","sarimax_forecast.to_csv(\"E:/Documents/TanXor/Model/forecasted_data/sarimax_forecast.csv\", header=True)\n","randomforest_forecast.to_csv(\"E:/Documents/TanXor/Model/forecasted_data/randomforest_forecast.csv\", header=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sarimax_forecast = pd.read_csv('E:/Documents/TanXor/Dataset/forecasted_data/sarimax_forecast.csv')\n","randomforest_forecast = pd.read_csv('E:/Documents/TanXor/Dataset/forecasted_data/randomforest_forecast.csv')\n","\n","model_eval = Evaluate(val_data.iloc[:,:2].sum(axis = 1))\n","\n","predictions = [sarimax_forecast['Total'], randomforest_forecast['Total']]\n","\n","for pred in predictions:\n","    print(model_eval.mape(pred))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["srx_best_ = {}\n","rfr_best_ = {}\n","\n","best_params_data = [srx_best_, rfr_best_]\n","\n","params_path = ['sarimax_best_params', 'rfr_best_params']\n","\n","for col in train_data.columns[:2]:\n","\n","    srx_hyperopt = SarimaxHyperopt(train_data[col], test_data[col])\n","\n","    rfr_optuna = RFR_Optuna(train_data[[col]], 7 , 0.2)\n","\n","    models = [srx_hyperopt, rfr_optuna]\n","\n","    for i, model in enumerate(models):\n","\n","        model.hyperparameter_tune(2)\n","\n","        best_params_data[i][col] = model.best_params\n","\n","for i, path in enumerate(params_path):\n","    save_to_json(path, best_params_data[i], 'w')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''\n","import dask\n","from dask import delayed\n","\n","## iterate over values and calculate the sum\n","for i, col in enumerate(train_data.columns[:2]):\n","    result = delayed(hyperparameter_tuning)(train_data, test_data, col, 2)\n","    out = result.compute()\n","'''"]},{"cell_type":"markdown","metadata":{},"source":["### Single Model Algorithm"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_params_data = {}\n","\n","for i in train_data.columns[:2]:\n","\n","    hp_model = SarimaxHyperopt(train_data[i], test_data[i])\n","\n","    hp_model.hyperparameter_tune(num_evals=3)\n","\n","    best = hp_model.best_params\n","\n","    input_data = {i: best}\n","\n","    best_params_data.update(input_data)\n","\n","save_to_json('sarimax_best_params', best_params_data, 'w')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_params = read_from_json('sarimax_best_params', mode=\"r\")\n","total_forecast = pd.DataFrame()\n","\n","for i in train_data.columns[:2]:\n","    model = SRX(train_data[i])\n","\n","    model.train(best_params[i])\n","\n","    forecast = model.predict(val_df[[i]])\n","\n","    total_forecast[f'Original_{i}'] = val_df[i]\n","\n","    total_forecast[f'Forecast_{i}'] = forecast.values\n","\n","total_forecast.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Feature Engineering"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creates a new Dataframe with the following columns:\n","# 1. Date, 2. Actual, 3. Lagged\n","seasonal_lag = Lag(train_data[[train_data.columns[2]]]).lag_transform(7, train_data.columns[2])\n","\n","# Creates a new Dataframe with the following columns:\n","# 1. Date, 2. Actual, 3. LogTransformed\n","log_transform = Log(train_data).log_transform(train_data.columns[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Differencing\n","# Adds a column to the dataframe that is the difference between the actual and lagged data\n","seasonal_lag['Seasonal_Diff'] = seasonal_lag.iloc[:,0] - seasonal_lag.iloc[:,1]\n","seasonal_lag = seasonal_lag.dropna()\n","seasonal_lag"]},{"cell_type":"markdown","metadata":{},"source":["### Stationarity Test"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-29T05:17:37.988344Z","iopub.status.busy":"2023-07-29T05:17:37.987582Z","iopub.status.idle":"2023-07-29T05:17:37.993579Z","shell.execute_reply":"2023-07-29T05:17:37.992678Z","shell.execute_reply.started":"2023-07-29T05:17:37.988309Z"},"trusted":true},"outputs":[],"source":["# Augmented Dickey-Fuller Test (Stationarity Test)\n","def adfuller_test(data):\n","    result=adfuller(data)\n","    return result[1] # Return p-value\n","\n","# Testing for stationarity on all columns\n","for i in train_data.columns[:70]:\n","    if (adfuller_test(train_data[i]) > 0.05):\n","        print(i, \"Fail\")\n","\n","        seasonal_lag = Lag(train_data[[i]]).lag_transform(7, i)\n","        seasonal_lag['Seasonal_Diff'] = seasonal_lag.iloc[:,0] - seasonal_lag.iloc[:,1]\n","        seasonal_lag = seasonal_lag.dropna()\n","        \n","        if (adfuller_test(seasonal_lag.iloc[:,-1]) > 0.05):\n","            print(i, \"Failed Again\")\n","        else:\n","            print(i, \"Passed\")       \n","        "]},{"cell_type":"markdown","metadata":{},"source":["## Model Training"]},{"cell_type":"markdown","metadata":{},"source":["### SARIMAX"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set model Parameters\n","params = {\n","    'p': 1, 'd': 1, 'q': 4,\n","    'P': 2, 'D': 1, 'Q': 3, 'm': 7\n","}\n","\n","col = test_data.columns[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-29T05:17:38.215617Z","iopub.status.idle":"2023-07-29T05:17:38.216436Z","shell.execute_reply":"2023-07-29T05:17:38.216187Z","shell.execute_reply.started":"2023-07-29T05:17:38.216159Z"},"trusted":true},"outputs":[],"source":["# Initialize the model with the training data\n","model = SRX(train_data[col])\n","\n","# Train the model on the given parameters\n","model.train(params)\n","\n","# Predict the values using the model\n","test_data['forecast']=model.predict(test_data[col])\n","\n","# Evaluate the model on the above predictions\n","print(model.evaluate())\n","\n","# Plot the forecast against the actuals\n","test_data[[col, 'forecast']].plot()"]},{"cell_type":"markdown","metadata":{},"source":["### Random Forest Regressor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rfr_params = {\n","    'n_estimators': 100,\n","    'max_depth': 25,\n","    'max_features':'log2',\n","    'min_samples_leaf': 10,\n","    'min_samples_split': 12,\n","    'bootstrap': False,\n","}\n","\n","col = test_data.columns[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = RFR(train_data[[col]])\n","\n","model.data_preprocess(7)\n","\n","model.train_test_split(0.2)\n","\n","model.fit(rfr_params)\n","\n","model.predict()\n","\n","rfr_mape = model.evaluate()\n","\n","print(f'MAPE: {rfr_mape:.2f}%')"]},{"cell_type":"markdown","metadata":{},"source":["## Hyperparameter Tuning"]},{"cell_type":"markdown","metadata":{},"source":["### SARIMAX_HYPEROPT"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initalize the model with the test and train data:\n","hp_model = SarimaxHyperopt(train_data[train_data.columns[0]], test_data[test_data.columns[0]])\n","\n","# Tune the hyperparameters:\n","hp_model.hyperparameter_tune(num_evals=3)\n","\n","# Returns the best parameters for the model\n","best = hp_model.best_params\n","print('Best parameters for the model: ', best)"]},{"cell_type":"markdown","metadata":{},"source":["### Random Forest Regressor Optuna"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initalize the model with the unprocessed raw data:\n","model = RFR_Optuna(train_data[[col]])\n","\n","# Add new lagged columns to the data:\n","model.data_preprocess(7)\n","\n","# Create the features and target:\n","model.train_test_split(0.2)\n","\n","# Hyperparameter tuning:\n","model.hyperparameter_tune()\n","\n","# Train the model:\n","model.fit()\n","\n","# Make predictions:\n","model.predict()\n","\n","# Evaluate the model:\n","rfr_mape = model.evaluate()\n","\n","print(f'MAPE: {rfr_mape:.2f}%')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred = mod.predict(X_test)\n","plt.rcParams[\"figure.figsize\"] = (12,8)\n","plt.plot(pred,label='Random_Forest_Predictions')\n","plt.plot(y_test,label='Actual Sales')\n","plt.legend(loc=\"upper left\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Custom Methods"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Custom Hyperparameter Tuning ###\n","'''\n","import itertools\n","\n","### Define Parameter Ranges to Test ###\n","\n","# Note: higher numbers will result in code taking much longer to run\n","# Here we have it set to test p,d,q each = 0, 1 & 2\n","\n","# Define the p, d and q parameters to take any value between 0 and 3 (exclusive)\n","p = range(1, 6)\n","q = range(0, 6)\n","d = range(1, 2)\n","P = range(0, 4)\n","Q = range(0, 4)\n","\n","# Generate all different combinations of p, q and q triplets\n","pdq = list(itertools.product(p, d, q))\n","\n","# Generate all different combinations of seasonal p, q and q triplets\n","# Note: here we have 12 in the 's' position as we have monthly data\n","# You'll want to change this according to your time series' frequency\n","pdqs = [(x[0], x[1], x[2], 7) for x in list(itertools.product(P, d, Q))]\n","\n","### Run Grid Search ###\n","\n","# Note: this code will take a while to run\n","\n","# Define function\n","def sarimax_gridsearch(ts, pdq, pdqs, freq='D'):\n","    \n","    Input: \n","        ts : your time series data\n","        pdq : ARIMA combinations from above\n","        pdqs : seasonal ARIMA combinations from above\n","        maxiter : number of iterations, increase if your model isn't converging\n","        frequency : default='M' for month. Change to suit your time series frequency\n","            e.g. 'D' for day, 'H' for hour, 'Y' for year. \n","        \n","    Return:\n","        Prints out top 5 parameter combinations\n","        Returns dataframe of parameter combinations ranked by BIC\n","    \n","\n","    # Run a grid search with pdq and seasonal pdq parameters and get the best BIC value\n","    ans = []\n","    for comb in pdq:\n","        for combs in pdqs:\n","            try:\n","                mod = sm.tsa.statespace.SARIMAX(ts, # this is your time series you will input\n","                                                order=comb,\n","                                                seasonal_order=combs,\n","                                                enforce_stationarity=False, \n","                                                enforce_invertibility=False,\n","                                                freq=freq)\n","\n","                output = mod.fit(maxiter=1000)\n","                predictions = output.predict(start=1800,end=1913,dynamic=True)\n","\n","                test_data = ts.iloc[1800:1913]\n","                mape = np.mean(np.abs((test_data - predictions) / test_data)) * 100\n","\n","                ans.append([comb, combs, output.bic, mape])\n","                print('SARIMAX {} x {}12 : MAPE Calculated ={}'.format(comb, combs, mape))\n","            except:\n","                continue\n","            \n","    # Find the parameters with minimal BIC value\n","\n","    # Convert into dataframe\n","    ans_df = pd.DataFrame(ans, columns=['pdq', 'pdqs', 'bic', 'mape'])\n","\n","    # Sort and return top 5 combinations\n","    ans_df = ans_df.sort_values(by=['mape'],ascending=True)\n","    \n","    return ans_df\n","    \n","\n","    ### Apply function to your time series data ###\n","#Remember to change frequency to match your time series data\n","best_params = sarimax_gridsearch(df_1['Sales'], pdq, pdqs, freq='D')\n","best_params.head(20)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Optuna Hyperparameter Optimization ###\n","'''\n","def objective(trial):\n","    p = trial.suggest_int('p', 0, 6)\n","    # d = trial.suggest_int('d', 1, 3)\n","    q = trial.suggest_int('q', 0, 6)\n","    P = trial.suggest_int('P', 0, 6)\n","    Q = trial.suggest_int('Q', 0, 6)\n","    # m = trial.suggest_int('m', 3, 8)\n","    srx = sm.tsa.statespace.SARIMAX(df_1['Sales'], \n","                                    order=(p,1,q), \n","                                    seasonal_order=(P,1,Q,7), \n","                                    enforce_stationarity=False, \n","                                    enforce_invertibility=False,\n","                                    freq='D')\n","    \n","    output = srx.fit(maxiter=1000)\n","    predictions = output.predict(start=1800,end=1913,dynamic=True)\n","\n","    test_data = df_1['Sales'].iloc[1800:1913]\n","    mape = np.mean(np.abs((test_data - predictions) / test_data)) * 100\n","\n","    return mape\n","\n","study = optuna.create_study(direction='minimize')\n","study.optimize(objective, n_trials=10)\n","'''"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"}},"nbformat":4,"nbformat_minor":4}
